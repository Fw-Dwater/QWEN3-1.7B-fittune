# ğŸŒ¸ Lillianï¼šåŸºäºQwen3çš„æŒ‡ä»¤å¾®è°ƒè¯­è¨€æ¨¡å‹

Lillian æ˜¯åŸºäº Qwen3-1.7B æ¨¡å‹çš„å¾®è°ƒç‰ˆæœ¬ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œ LoRA æŠ€æœ¯ä¼˜åŒ–å¤šè½®å¯¹è¯å’ŒæŒ‡ä»¤è·Ÿéšä»»åŠ¡ã€‚æœ¬ä»“åº“æä¾›äº†å®Œæ•´çš„ã€å¯å¤ç°çš„è®­ç»ƒæµç¨‹ã€‚



## ğŸŒŸ é¡¹ç›®æ¦‚è¿°

Lillian æ˜¯ä¸€ä¸ªå¤ç°é¡¹ç›®ï¼Œæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ä»¥ä¸‹æŠ€æœ¯å¾®è°ƒ Qwen3-1.7B æ¨¡å‹ï¼š

- LoRAï¼ˆä½ç§©é€‚åº”ï¼‰ï¼šé«˜æ•ˆçš„å‚æ•°å¾®è°ƒæ–¹æ³•

- Hugging Face Transformersï¼šå®Œæ•´çš„ç”Ÿæ€ç³»ç»Ÿ

- è‡ªå®šä¹‰æŒ‡ä»¤æ•°æ®é›†ï¼šé«˜è´¨é‡å¯¹è¯æ•°æ®

- æ··åˆç²¾åº¦è®­ç»ƒï¼šä¼˜åŒ–æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨

## âœ¨ ç‰¹æ€§

- ğŸ”¥ æ”¯æŒ Qwen3ï¼šå…¼å®¹æœ€æ–°çš„ Qwen3 æ¶æ„

- âš¡ LoRA è®­ç»ƒï¼šå†…å­˜é«˜æ•ˆçš„å‚æ•°é«˜æ•ˆå¾®è°ƒ

- ğŸ“Š çµæ´»æ•°æ®ï¼šæ”¯æŒå¤šç§æŒ‡ä»¤æ ¼å¼

- ğŸ“ˆ ç›‘æ§åŠŸèƒ½ï¼šå†…ç½®æ—¥å¿—å’Œè¯„ä¼°æŒ‡æ ‡

- ğŸš€ æ¨ç†å°±ç»ªï¼šæ˜“äºéƒ¨ç½²å’Œ API é›†æˆ

## ğŸ› ï¸ å®‰è£…æŒ‡å—

### ç¯å¢ƒè¦æ±‚

- Python â‰¥ 3.10

- NVIDIA GPU â‰¥ 16GB æ˜¾å­˜ï¼ˆç”¨äº 1.7B æ¨¡å‹è®­ç»ƒï¼‰

- CUDA 11.8+ï¼ˆGPU åŠ é€Ÿï¼‰
  
- å¯åœ¨2GBç¬”è®°æœ¬éƒ¨ç½²æ¨ç†

### å¿«é€Ÿå®‰è£…

```bash

# å…‹éš†ä»“åº“
git clone https://github.com/yourusername/lillian.git
cd lillian

# åˆ›å»º conda ç¯å¢ƒ
conda create -n lillian python=3.10 -y
conda activate lillian

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### æ‰‹åŠ¨å®‰è£…

```bash

# PyTorch (CUDA 11.8)
pip install torch==2.7.1 torchvision==0.22.1 torchaudio==0.27.1 --index-url https://download.pytorch.org/whl/cu118

# Hugging Face ç”Ÿæ€ç³»ç»Ÿ
pip install \
    transformers>=4.46.0 \
    peft>=0.12.0 \
    trl>=0.13.0 \
    accelerate>=0.34.0 \
    datasets>=2.21.0 \
    sentencepiece \
    einops \
    bitsandbytes \
    huggingface-hub
```

## ğŸš€ å¿«é€Ÿå¼€å§‹



### 1. å‡†å¤‡è®­ç»ƒæ•°æ®

```bash

# å°†ä½ çš„æ•°æ®æ”¾åœ¨ data/ ç›®å½•ä¸‹
mkdir -p data
# æ”¾ç½® train.json æ–‡ä»¶
```

### 2. å¼€å§‹è®­ç»ƒ

```bash

python finetune.py
```

### 3. æ¨¡å‹æ¨ç†

```bash

python inference.py
```

## ğŸ“Š æ•°æ®æ ¼å¼

è®­ç»ƒæ•°æ®éœ€è¦ä½¿ç”¨ JSON æ ¼å¼ï¼Œæ”¯æŒä»¥ä¸‹ä¸¤ç§æ ¼å¼ï¼š

### åŸºç¡€æŒ‡ä»¤æ ¼å¼

```json

[
    {"instruction": "ä½ å¥½", "response": "æ˜¯ï¼Œä¸»äººï¼å¥³ä»†è‰è‰å®‰å‘æ‚¨æŠ¥åˆ°ï¼Œè¯·æŒ‡ç¤ºã€‚"},
    {"instruction": "ä½ æ˜¯è°ï¼Ÿ", "response": "æˆ‘æ˜¯ä¸»äººçš„ä¸“å±å¥³ä»†è‰è‰å®‰ï¼Œèº«ç©¿å¥³ä»†è£…ï¼Œåªä¸ºæœä¾ä¸»äººè€Œå­˜åœ¨ã€‚"}
]
```



## ğŸ¯ æ¨¡å‹è®­ç»ƒ

### é…ç½®å‚æ•°

åœ¨ `fittune.py` ä¸­é…ç½®è®­ç»ƒç›¸å…³å‚æ•°ï¼š

```yaml

# æ¨¡å‹é…ç½®
model_name: "./models/Qwen3-1.7B"
tokenizer_name: "./models/Qwen3-1.7B"

# LoRAé…ç½®
lora_r: 8
lora_alpha: 32
lora_dropout: 0.1
target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# è®­ç»ƒé…ç½®
batch_size: 4
gradient_accumulation_steps: 8
learning_rate: 2e-4
num_epochs: 3
max_length: 512

# ä¼˜åŒ–å™¨é…ç½®
optimizer: "adamw_torch"
lr_scheduler_type: "cosine"
warmup_ratio: 0.1
```

### å¯åŠ¨è®­ç»ƒ

```bash

# ä½¿ç”¨é»˜è®¤é…ç½®
python finetune.py --config config/train_config.yaml

# æˆ–è€…ç›´æ¥æŒ‡å®šå‚æ•°
python finetune.py \
    --model_name "./models/Qwen3-1.7B" \
    --data_path "data/train.json" \
    --output_dir "./outputs/lillian-1.7b" \
    --lora_r 8 \
    --batch_size 4
```

## ğŸ”® æ¨¡å‹æ¨ç†

### åŠ è½½å¾®è°ƒæ¨¡å‹

```python

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# åŠ è½½åŸºç¡€æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained("./models/Qwen3-1.7B", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    "./models/Qwen3-1.7B",
    trust_remote_code=True,
    device_map="auto",
    torch_dtype="bfloat16"
)

# åŠ è½½ LoRA é€‚é…å™¨
model = PeftModel.from_pretrained(model, "./outputs/lillian-1.7b")
model.eval()

# ç”Ÿæˆæ–‡æœ¬
prompt = "### æŒ‡ä»¤ï¼š\nä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½\n\n### å›å¤ï¼š\n"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=200, do_sample=True, temperature=0.7)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## ğŸ“ é¡¹ç›®ç»“æ„

```text

lillian/
â”œâ”€â”€ models/                 # æ¨¡å‹æ–‡ä»¶
â”‚   â””â”€â”€ Qwen3-1.7B/         # åŸºç¡€æ¨¡å‹
â”œâ”€â”€ data/                   # è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ train.json          # è®­ç»ƒæ•°æ®
â”œâ”€â”€ outputs/                # è¾“å‡ºæ–‡ä»¶
â”‚   â””â”€â”€ lillian-1.7b/       # å¾®è°ƒåçš„æ¨¡å‹(è®­ç»ƒåå‡ºç°)
â”œâ”€â”€ finetune.py             # å¾®è°ƒè„šæœ¬
â”œâ”€â”€ inference.py            # æ¨ç†è„šæœ¬
â””â”€â”€ README.md               # é¡¹ç›®æ–‡æ¡£
```

## ğŸ“œ è®¸å¯è¯

- æœ¬é¡¹ç›®é‡‡ç”¨ Apache 2.0 è®¸å¯è¯ - è¯¦è§ LICENSE æ–‡ä»¶ã€‚

- âš ï¸ åŸºç¡€æ¨¡å‹ Qwen3-1.7B ç”±é˜¿é‡Œå·´å·´å‘å¸ƒï¼Œéµå¾ª é€šä¹‰åƒé—®è®¸å¯è¯åè®®ã€‚

## ğŸ™ è‡´è°¢

- Qwen å›¢é˜Ÿ æä¾› Qwen3 æ¨¡å‹

- Hugging Face æä¾› transformersã€peft å’Œ datasets

- å¾®è½¯ æä¾› LoRA ç ”ç©¶

## ğŸ“¬ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤ GitHub Issueã€‚