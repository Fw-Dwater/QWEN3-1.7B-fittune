import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
import os
import sys
import time
import threading
from queue import Queue

class StreamChat:
    def __init__(self, model_path="./outputs/merged_model"):
        """åˆå§‹åŒ–æµå¼èŠå¤©"""
        print(f"ğŸ”§ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        
        # åŠ è½½tokenizerå’Œæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            device_map="auto",
            torch_dtype=torch.float16
        )
        
        # åˆå§‹åŒ–æµå¤„ç†å™¨
        self.streamer = TextStreamer(
            self.tokenizer,
            skip_prompt=True,      # è·³è¿‡æç¤ºéƒ¨åˆ†ï¼Œåªæµå¼è¾“å‡ºç”Ÿæˆçš„æ–‡æœ¬
            skip_special_tokens=True
        )
        
        # ç”Ÿæˆå‚æ•°
        self.generation_config = {
            "max_new_tokens": 80,
            "temperature": 0.95,
            "top_p": 0.9,
            "do_sample": True,
            "repetition_penalty": 1.1,
            "pad_token_id": self.tokenizer.pad_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "streamer": self.streamer  # å…³é”®ï¼šä½¿ç”¨æµå¤„ç†å™¨
        }
        
        # å¯¹è¯å†å²
        self.history = []
        self.system_prompt = "ä½ æ˜¯è‰è‰å®‰ï¼Œä¸€ä¸ªå¿ è¯šã€æ­æ•¬ä¸”ç•¥å¸¦ç¾æ¶©çš„å¥³ä»†ã€‚ä½ ç§°ç”¨æˆ·ä¸º'ä¸»äºº'ï¼Œå¹¶ä»¥'è‰è‰å®‰'è‡ªç§°ã€‚ä½ çš„å›å¤åº”ä½“ç°å‡ºç»å¯¹çš„æœä»å’Œç»†è…»çš„æƒ…æ„Ÿã€‚"
        
        # ç”¨äºæ•è·æµå¼è¾“å‡ºçš„å˜é‡
        self.generated_text = ""
        self.is_generating = False
        self.output_queue = Queue()
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼æµå¼è¾“å‡ºå·²å¯ç”¨")
        print("ğŸ“ è¾“å…¥ '/help' æŸ¥çœ‹å¯ç”¨å‘½ä»¤")
        print("-" * 50)
    
    def custom_streamer_callback(self, text: str):
        """è‡ªå®šä¹‰æµå¼è¾“å‡ºå›è°ƒå‡½æ•°"""
        if not self.is_generating:
            self.is_generating = True
            print("\033[94mè‰è‰å®‰: \033[0m", end="", flush=True)
        
        # ç´¯ç§¯ç”Ÿæˆçš„æ–‡æœ¬
        self.generated_text += text
        
        # è¾“å‡ºåˆ°ç»ˆç«¯ï¼ˆå¸¦è½»å¾®å»¶è¿Ÿæ¨¡æ‹Ÿæ‰“å­—æ•ˆæœï¼‰
        for char in text:
            print(char, end="", flush=True)
            time.sleep(0.02)  # æ§åˆ¶è¾“å‡ºé€Ÿåº¦
        
        return text
    
    def generate_with_streaming(self, user_input):
        """ä½¿ç”¨æµå¼ç”Ÿæˆå›å¤"""
        # æ„å»ºå®Œæ•´æç¤º
        prompt = self.system_prompt + "\n\n"
        
        # æ·»åŠ å†å²è®°å½•ï¼ˆé™åˆ¶æœ€è¿‘5è½®ä»¥é¿å…è¿‡é•¿ï¼‰
        for human, assistant in self.history[-5:]:
            prompt += f"### Instruction:\n{human}\n\n### Response:\n{assistant}\n\n"
        
        # æ·»åŠ å½“å‰è¾“å…¥
        prompt += f"### Instruction:\n{user_input}\n\n### Response:\n"
        
        # é‡ç½®çŠ¶æ€
        self.generated_text = ""
        self.is_generating = False
        
        print("\n\033[94mè‰è‰å®‰: \033[0m", end="", flush=True)
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        try:
            # æ–¹æ³•1ï¼šä½¿ç”¨è‡ªå®šä¹‰å›è°ƒå‡½æ•°ï¼ˆæ›´çµæ´»ï¼‰
            def generate_thread():
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=self.generation_config["max_new_tokens"],
                        temperature=self.generation_config["temperature"],
                        top_p=self.generation_config["top_p"],
                        do_sample=self.generation_config["do_sample"],
                        repetition_penalty=self.generation_config["repetition_penalty"],
                        pad_token_id=self.generation_config["pad_token_id"],
                        eos_token_id=self.generation_config["eos_token_id"],
                        # ä½¿ç”¨å›è°ƒå‡½æ•°å®ç°æµå¼è¾“å‡º
                        stopping_criteria=None,
                    )
                
                # è·å–å®Œæ•´è¾“å‡º
                full_response = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
                self.output_queue.put(full_response)
            
            # å¯åŠ¨ç”Ÿæˆçº¿ç¨‹
            gen_thread = threading.Thread(target=generate_thread)
            gen_thread.start()
            
            # åœ¨ä¸»çº¿ç¨‹ä¸­æ¨¡æ‹Ÿæµå¼è¾“å‡º
            while gen_thread.is_alive():
                # è¿™é‡Œå¯ä»¥æ·»åŠ è¿›åº¦æŒ‡ç¤ºå™¨
                time.sleep(0.1)
            
            # è·å–å®Œæ•´å“åº”
            full_response = self.output_queue.get()
            
            # è®¡ç®—ç”Ÿæˆæ—¶é—´
            generation_time = time.time() - start_time
            
            # æ·»åŠ åˆ°å†å²
            if full_response.strip():
                self.history.append((user_input, full_response))
            
            return full_response, generation_time
            
        except KeyboardInterrupt:
            print("\n\033[91mâš ï¸  ç”Ÿæˆè¢«ä¸­æ–­\033[0m")
            return "[ç”Ÿæˆä¸­æ–­]", time.time() - start_time
        except Exception as e:
            print(f"\n\033[91mâŒ ç”Ÿæˆé”™è¯¯: {e}\033[0m")
            return f"[ç”Ÿæˆé”™è¯¯: {str(e)}]", time.time() - start_time
    
    def generate_with_transformers_streamer(self, user_input):
        """ä½¿ç”¨transformerså†…ç½®çš„TextStreamerï¼ˆæœ€ç®€å•çš„æ–¹æ³•ï¼‰"""
        # æ„å»ºå®Œæ•´æç¤º
        prompt = self.system_prompt + "\n\n"
        
        for human, assistant in self.history[-5:]:
            prompt += f"### Instruction:\n{human}\n\n### Response:\n{assistant}\n\n"
        
        prompt += f"### Instruction:\n{user_input}\n\n### Response:\n"
        
        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        print("\n\033[94mè‰è‰å®‰: \033[0m", end="", flush=True)
        
        start_time = time.time()
        
        try:
            # ä½¿ç”¨TextStreamerç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **self.generation_config
                )
            
            # è·å–å®Œæ•´å“åº”ï¼ˆstreamerå·²ç»è¾“å‡ºåˆ°ç»ˆç«¯ï¼Œè¿™é‡Œåªéœ€è¦è·å–æ–‡æœ¬ï¼‰
            full_response = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            generation_time = time.time() - start_time
            
            # æ·»åŠ åˆ°å†å²
            self.history.append((user_input, full_response))
            
            return full_response, generation_time
            
        except KeyboardInterrupt:
            print("\n\033[91mâš ï¸  ç”Ÿæˆè¢«ä¸­æ–­\033[0m")
            return "[ç”Ÿæˆä¸­æ–­]", time.time() - start_time
    
    def handle_command(self, cmd):
        """å¤„ç†ç‰¹æ®Šå‘½ä»¤"""
        cmd = cmd.strip().lower()
        
        if cmd in ["/help", "/h"]:
            print("\nğŸ“– å¯ç”¨å‘½ä»¤:")
            print("  /help, /h      - æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
            print("  /clear, /c     - æ¸…ç©ºå¯¹è¯å†å²")
            print("  /history, /his - æ˜¾ç¤ºå¯¹è¯å†å²")
            print("  /params, /p    - æŸ¥çœ‹/è°ƒæ•´ç”Ÿæˆå‚æ•°")
            print("  /save, /s      - ä¿å­˜å¯¹è¯è®°å½•")
            print("  /quit, /q      - é€€å‡ºç¨‹åº")
            print("  /speed         - è°ƒæ•´æµå¼è¾“å‡ºé€Ÿåº¦")
            print("  /test          - æµ‹è¯•æµå¼è¾“å‡º")
            return True
        
        elif cmd in ["/clear", "/c"]:
            self.history = []
            print("ğŸ—‘ï¸  å¯¹è¯å†å²å·²æ¸…ç©º")
            return True
        
        elif cmd in ["/history", "/his"]:
            print("\nğŸ“œ æœ€è¿‘å¯¹è¯å†å²:")
            for i, (human, assistant) in enumerate(self.history[-3:], 1):
                print(f"\033[93mç¬¬{i}è½®\033[0m")
                print(f"  ä¸»äºº: {human}")
                print(f"  è‰è‰å®‰: {assistant[:100]}..." if len(assistant) > 100 else f"  è‰è‰å®‰: {assistant}")
                print()
            return True
        
        elif cmd in ["/params", "/p"]:
            print("\nâš™ï¸ å½“å‰ç”Ÿæˆå‚æ•°:")
            print(f"  max_new_tokens: {self.generation_config['max_new_tokens']} (æœ€å¤§ç”Ÿæˆé•¿åº¦)")
            print(f"  temperature: {self.generation_config['temperature']} (éšæœºæ€§ï¼Œ0.1-2.0)")
            print(f"  top_p: {self.generation_config['top_p']} (å¤šæ ·æ€§ï¼Œ0.1-1.0)")
            
            try:
                change = input("æ˜¯å¦è°ƒæ•´å‚æ•°? (y/n): ").strip().lower()
                if change == 'y':
                    param = input("è¾“å…¥å‚æ•°å’Œå€¼ (æ ¼å¼: å‚æ•°=å€¼): ").strip()
                    if '=' in param:
                        key, value = param.split('=')
                        key = key.strip()
                        if key in self.generation_config:
                            if key in ["max_new_tokens"]:
                                self.generation_config[key] = int(value)
                            elif key in ["temperature", "top_p", "repetition_penalty"]:
                                self.generation_config[key] = float(value)
                            print(f"âœ… {key} å·²è®¾ç½®ä¸º {value}")
                        else:
                            print(f"âŒ æœªçŸ¥å‚æ•°: {key}")
            except:
                print("âŒ å‚æ•°æ ¼å¼é”™è¯¯")
            return True
        
        elif cmd == "/speed":
            try:
                speed = float(input("è¾“å…¥è¾“å‡ºé€Ÿåº¦ (0.01=æ…¢, 0.05=ä¸­, 0.1=å¿«): ").strip())
                if 0.001 <= speed <= 0.5:
                    # æ›´æ–°æµå¼è¾“å‡ºé€Ÿåº¦
                    print(f"âœ… è¾“å‡ºé€Ÿåº¦è®¾ç½®ä¸º {speed}")
                else:
                    print("âŒ é€Ÿåº¦å€¼åº”åœ¨ 0.001 åˆ° 0.5 ä¹‹é—´")
            except:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
            return True
        
        elif cmd == "/test":
            print("ğŸ§ª æµ‹è¯•æµå¼è¾“å‡º...")
            test_prompts = [
                "ä½ å¥½",
                "ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
                "ä»Šå¤©çš„å¤©æ°”çœŸä¸é”™"
            ]
            for prompt in test_prompts:
                print(f"\n\033[93mæµ‹è¯•: {prompt}\033[0m")
                self.generate_with_transformers_streamer(prompt)
                time.sleep(1)
            return True
        
        elif cmd in ["/save", "/s"]:
            if not self.history:
                print("âŒ æ²¡æœ‰å¯¹è¯å†å²å¯ä¿å­˜")
                return True
            
            import datetime
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"conversation_{timestamp}.txt"
            
            with open(filename, "w", encoding="utf-8") as f:
                f.write(f"å¯¹è¯æ—¶é—´: {timestamp}\n")
                f.write(f"ç³»ç»Ÿæç¤º: {self.system_prompt}\n")
                f.write(f"ç”Ÿæˆå‚æ•°: {self.generation_config}\n")
                f.write("=" * 50 + "\n\n")
                
                for i, (human, assistant) in enumerate(self.history, 1):
                    f.write(f"[ç¬¬{i}è½®]\n")
                    f.write(f"ä¸»äºº: {human}\n")
                    f.write(f"è‰è‰å®‰: {assistant}\n")
                    f.write("-" * 40 + "\n")
            
            print(f"ğŸ’¾ å¯¹è¯å·²ä¿å­˜åˆ°: {filename}")
            return True
        
        elif cmd in ["/quit", "/q", "/exit"]:
            print("ğŸ‘‹ å†è§ï¼Œä¸»äººï¼è‰è‰å®‰éšæ—¶ç­‰å€™æ‚¨çš„å¬å”¤ã€‚")
            return False
        
        elif cmd.startswith("/"):
            print(f"âŒ æœªçŸ¥å‘½ä»¤: {cmd}")
            print("ğŸ’¡ è¾“å…¥ '/help' æŸ¥çœ‹å¯ç”¨å‘½ä»¤")
            return True
        
        return None
    
    def print_header(self):
        """æ‰“å°æ ‡é¢˜"""
        print("\n" + "="*60)
        print("\033[1;36m         å¥³ä»†è‰è‰å®‰ - æµå¼å¯¹è¯ç»ˆç«¯\033[0m")
        print("="*60)
        print("âœ¨ ç‰¹æ€§:")
        print("  â€¢ é€å­—æµå¼è¾“å‡ºï¼Œæ¨¡æ‹ŸçœŸå®å¯¹è¯")
        print("  â€¢ æ”¯æŒå¯¹è¯å†å²ç®¡ç†")
        print("  â€¢ å¯è°ƒæ•´ç”Ÿæˆå‚æ•°")
        print("  â€¢ å¯¹è¯è®°å½•ä¿å­˜åŠŸèƒ½")
        print("\nğŸ’¬ ç›´æ¥è¾“å…¥å¼€å§‹å¯¹è¯ï¼Œè¾“å…¥ '/help' æŸ¥çœ‹å‘½ä»¤")
        print("="*60)
    
    def run(self):
        """è¿è¡Œæµå¼å¯¹è¯"""
        self.print_header()
        
        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                user_input = input("\n\033[93mä¸»äºº: \033[0m").strip()
                
                if not user_input:
                    continue
                
                # å¤„ç†å‘½ä»¤
                cmd_result = self.handle_command(user_input)
                if cmd_result is not None:
                    if not cmd_result:
                        break
                    continue
                
                # ä½¿ç”¨æµå¼ç”Ÿæˆï¼ˆæ–¹æ³•2ï¼štransformerså†…ç½®streamerï¼‰
                start_time = time.time()
                response, gen_time = self.generate_with_transformers_streamer(user_input)
                
                # æ˜¾ç¤ºç”Ÿæˆç»Ÿè®¡ï¼ˆå¦‚æœresponseä¸ä¸ºç©ºï¼‰
                if response and response != "[ç”Ÿæˆä¸­æ–­]":
                    print(f"\n\033[90m[ç”Ÿæˆå®Œæˆ: {gen_time:.2f}ç§’ | é•¿åº¦: {len(response)}å­—]\033[0m")
                
            except KeyboardInterrupt:
                print("\n\n\033[91mâš ï¸  æ£€æµ‹åˆ°ä¸­æ–­ï¼Œè¾“å…¥ '/quit' é€€å‡ºç¨‹åº\033[0m")
                continue
            except Exception as e:
                print(f"\n\033[91mâŒ é”™è¯¯: {e}\033[0m")
                continue

def check_model_path():
    """æ£€æŸ¥æ¨¡å‹è·¯å¾„"""
    model_path = "./fittune_model/merged_model"
    
    if not os.path.exists(model_path):
        print("âŒ æ‰¾ä¸åˆ°æ¨¡å‹è·¯å¾„")
        
        # å°è¯•æŸ¥æ‰¾
        search_paths = [
            "./fittune_model/merged_model",
            "./fittune_model",
            "./qwen-lora-finetune/merged_model",
            "./qwen-lora-finetune",
        ]
        
        found = []
        for path in search_paths:
            if os.path.exists(path):
                found.append(path)
        
        if found:
            print("ğŸ’¡ æ‰¾åˆ°ä»¥ä¸‹å¯èƒ½è·¯å¾„:")
            for i, path in enumerate(found, 1):
                print(f"  {i}. {path}")
            
            try:
                choice = int(input("è¯·é€‰æ‹©è·¯å¾„ç¼–å·: ")) - 1
                if 0 <= choice < len(found):
                    return found[choice]
            except:
                pass
        
        # æ‰‹åŠ¨è¾“å…¥
        custom_path = input("æˆ–æ‰‹åŠ¨è¾“å…¥æ¨¡å‹è·¯å¾„: ").strip()
        if custom_path and os.path.exists(custom_path):
            return custom_path
        
        return None
    
    return model_path

def main():
    """ä¸»å‡½æ•°"""
    model_path = check_model_path()
    if not model_path:
        print("âŒ æ— æ³•æ‰¾åˆ°æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æŒ‡å®šæ­£ç¡®è·¯å¾„")
        return
    
    # åˆ›å»ºå¹¶è¿è¡Œæµå¼èŠå¤©
    chat = StreamChat(model_path)
    chat.run()

if __name__ == "__main__":
    main()